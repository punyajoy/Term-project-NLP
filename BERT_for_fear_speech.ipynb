{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_codes.feature_generation import combine_features,return_dataloader,return_dataloader_inference,return_cnngru_dataloader\n",
    "from bert_codes.data_extractor import data_collector\n",
    "from bert_codes.own_bert_models import *\n",
    "from bert_codes.utils import *\n",
    "from transformers import *\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from sklearn.metrics import accuracy_score,f1_score\n",
    "from utils_function import mapping_to_actual_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 GPU(s) available.\n",
      "We will use the GPU: Tesla P100-PCIE-16GB\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():    \n",
    "\t# Tell PyTorch to use the GPU.    \n",
    "\tdevice = torch.device(\"cuda\")\n",
    "\tprint('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\tprint('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "# If not...\n",
    "else:\n",
    "\tprint('No GPU available, using the CPU instead.')\n",
    "\tdevice = torch.device(\"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={\n",
    "    'max_length':128,\n",
    "    'path_files': 'models_saved/mbert_fearspeech/',\n",
    "    'what_bert':'normal',\n",
    "    'batch_size':32,\n",
    "    'is_train':True,\n",
    "    'learning_rate':2e-5,\n",
    "    'epsilon':1e-8,\n",
    "    'random_seed':2020,\n",
    "    'weights':[1.0,9.0],\n",
    "    'epochs':5\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BERT_for_inference(params,total_data=None):\n",
    "    tokenizer = BertTokenizer.from_pretrained(params['path_files'], do_lower_case=False)\n",
    "    model=select_model(params['what_bert'],params['path_files'],params['weights'])\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    all_sentences = total_data.text    \n",
    "    input_total_ids,att_masks_total=combine_features(all_sentences,tokenizer,params['max_length'],\n",
    "                                                     take_pair=False,take_target=False)\n",
    "    train_dataloader = return_dataloader_inference(input_total_ids,att_masks_total,batch_size=params['batch_size'],is_train=False)\n",
    "    \n",
    "    softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    pred_labels=[]\n",
    "    pred_probab=[]\n",
    "    \n",
    "    for step, batch in tqdm(enumerate(train_dataloader)):\n",
    "        # Add batch to GPU\n",
    "        t0 = time.time()\n",
    "\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask = batch\n",
    "        # Telling the model not to compute or store gradients, saving memory and\n",
    "        # speeding up validation\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(b_input_ids, \n",
    "                            token_type_ids=None, \n",
    "                            attention_mask=b_input_mask)\n",
    "\n",
    "        logits = outputs[0]\n",
    "        logits = softmax(logits)\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        # Accumulate the total accuracy.\n",
    "        pred_labels+=list(np.argmax(logits, axis=1).flatten())\n",
    "        pred_probab+=list([ele[1] for ele in logits])\n",
    "        \n",
    "        # Track the number of batches\n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    total_data['preds']=pred_labels\n",
    "    total_data['pred_probab']=pred_probab\n",
    "    print(\" Test took: {:}\".format(format_time(time.time() - t0)))\n",
    "    return total_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data=pd.read_pickle('../../Data/data_to_be_annotated.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data_filter=total_data[total_data['keywords_count']>1]\n",
    "total_data_left=total_data[total_data['keywords_count']<=1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29091"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(total_data_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0427 18:52:50.893791 140362865473344 tokenization_utils.py:335] Model name 'models_saved/mbert_fearspeech/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming 'models_saved/mbert_fearspeech/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0427 18:52:50.895341 140362865473344 tokenization_utils.py:364] Didn't find file models_saved/mbert_fearspeech/added_tokens.json. We won't load it.\n",
      "I0427 18:52:50.896470 140362865473344 tokenization_utils.py:416] loading file models_saved/mbert_fearspeech/vocab.txt\n",
      "I0427 18:52:50.897231 140362865473344 tokenization_utils.py:416] loading file None\n",
      "I0427 18:52:50.897938 140362865473344 tokenization_utils.py:416] loading file models_saved/mbert_fearspeech/special_tokens_map.json\n",
      "I0427 18:52:50.898618 140362865473344 tokenization_utils.py:416] loading file models_saved/mbert_fearspeech/tokenizer_config.json\n",
      "I0427 18:52:51.008265 140362865473344 configuration_utils.py:231] loading configuration file models_saved/mbert_fearspeech/config.json\n",
      "I0427 18:52:51.009585 140362865473344 configuration_utils.py:256] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 105879\n",
      "}\n",
      "\n",
      "I0427 18:52:51.010659 140362865473344 modeling_utils.py:438] loading weights file models_saved/mbert_fearspeech/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing in fear\n",
      "Input shape before truncating [[101, 100, 11483, 100, 569, 31614, 35877, 44779, 10390, 142, 21809, 566, 15807, 11231, 100, 117, 102, 100, 100, 21426, 100, 100, 100, 100, 100, 579, 11231, 73893, 13610, 534, 45498, 11354, 100, 119, 102], [101, 84859, 100, 85234, 570, 13560, 27185, 100, 29416, 64188, 52557, 100, 100, 170, 100, 100, 100, 11263, 119, 119, 119, 119, 119, 119, 119, 119, 119, 100, 87410, 100, 170, 170, 100, 100, 100, 100, 100, 118, 100, 11142, 73349, 49946, 16985, 11263, 170, 100, 553, 91680, 10824, 95691, 37120, 10949, 100, 100, 100, 576, 72292, 52460, 10944, 577, 16985, 11263, 170, 170, 102, 100, 100, 11263, 11263, 100, 100, 170, 100, 85234, 570, 13560, 27185, 100, 29416, 64188, 52557, 100, 100, 170, 170, 100, 100, 100, 100, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 15513, 100, 100, 83329, 37524, 100, 170, 24137, 100, 100, 100, 48413, 119, 119, 119, 119, 119, 100, 85234, 54022, 10949, 100, 170, 170, 100, 100, 100, 11263, 102], [101, 100, 47505, 70751, 10949, 100, 10944, 565, 68048, 10824, 118, 67497, 100, 58358, 21426, 100, 100, 100, 19648, 56408, 44797, 10824, 100, 100, 100, 100, 100, 138, 100, 100, 571, 88706, 13036, 100, 140, 88840, 100, 117, 100, 117, 569, 13142, 100, 14500, 100, 118, 541, 118, 100, 11263, 47505, 70751, 10949, 571, 43329, 15678, 17068, 100, 532, 14256, 21310, 100, 100, 548, 102, 535, 100, 70843, 14619, 100, 117, 100, 100, 50010, 100, 100, 16288, 100, 13465, 100, 100, 119, 100, 100, 11483, 559, 27843, 13163, 24820, 36335, 100, 117, 11384, 30952, 48413, 100, 569, 80969, 16129, 100, 100, 52061, 100, 119, 100, 100, 100, 47505, 70751, 10949, 569, 47786, 36335, 100, 14500, 73282, 569, 80969, 16129, 100, 12334, 566, 58246, 58358, 18155, 100, 100, 102], [101, 579, 35872, 117, 81508, 35872, 117, 100, 117, 573, 38464, 13764, 117, 52326, 70891, 96807, 16130, 18660, 100, 564, 11468, 100, 64883, 12334, 100, 100, 576, 68309, 100, 100, 100, 591, 100, 100, 64188, 100, 100, 81508, 87410, 100, 100, 14500, 62393, 100, 100, 91752, 576, 68309, 100, 21426, 571, 11231, 23562, 11483, 573, 66396, 10949, 24820, 27079, 100, 591, 13859, 68961, 85185, 102, 100, 57610, 11142, 100, 10944, 100, 54022, 10824, 21426, 100, 23974, 100, 43828, 100, 11263, 14473, 84119, 591, 85185, 536, 43409, 100, 14500, 30017, 100, 11142, 100, 100, 100, 10944, 62442, 11483, 100, 100, 100, 11142, 100, 580, 13036, 117, 100, 100, 100, 580, 13036, 11142, 100, 100, 14473, 85066, 49997, 100, 117, 576, 11231, 13610, 110, 10944, 100, 100, 591, 13901, 102], [101, 10091, 100, 100, 100, 100, 100, 12334, 79171, 10824, 100, 100, 100, 100, 54022, 70238, 12030, 20505, 25662, 573, 20705, 52580, 11483, 70345, 100, 566, 68309, 30837, 14668, 100, 100, 10944, 100, 11483, 100, 100, 566, 37208, 100, 118, 549, 41469, 41469, 38464, 100, 29416, 100, 100, 100, 100, 38740, 52061, 15721, 56408, 14473, 13465, 36335, 14187, 76167, 53836, 36666, 19713, 119, 119, 102, 14619, 100, 119, 91039, 23067, 11641, 61474, 100, 100, 100, 100, 100, 30441, 14187, 14500, 117, 100, 14668, 11483, 100, 100, 100, 100, 100, 119, 27861, 11483, 13610, 14668, 11483, 11384, 11142, 100, 100, 100, 10944, 11483, 17109, 54210, 100, 15628, 10824, 61021, 117, 100, 13859, 14668, 11142, 100, 11263, 100, 579, 80969, 13036, 84119, 119, 100, 100, 53128, 96807, 11551, 100, 102]]\n",
      "(29091, 128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "910it [02:23,  6.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Test took: 0:00:05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/punyajoy/.conda/envs/punyajoy_gpu/lib/python3.7/site-packages/ipykernel_launcher.py:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/punyajoy/.conda/envs/punyajoy_gpu/lib/python3.7/site-packages/ipykernel_launcher.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "total_dataframe=BERT_for_inference(params,total_data_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/punyajoy/.conda/envs/punyajoy_gpu/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/punyajoy/.conda/envs/punyajoy_gpu/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "total_data_left['preds']=list(np.zeros((len(total_data_left)),dtype=int))\n",
    "total_data_left['pred_probab']=list(np.zeros((len(total_data_left)),dtype=float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=pd.concat([total_dataframe,total_data_left],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'total_data_' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-7c20722fe4f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtotal_data_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'total_data_' is not defined"
     ]
    }
   ],
   "source": [
    "total_data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data_to_annnotate=pd.read_pickle('../../Data/data_to_be_annotated.pkl')\n",
    "actual_data=pd.read_csv('../../Data/new_data_lang_without_spam_translated.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-punyajoy_gpu] *",
   "language": "python",
   "name": "conda-env-.conda-punyajoy_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_codes.feature_generation import combine_features,return_dataloader,return_dataloader_inference,return_cnngru_dataloader\n",
    "from bert_codes.data_extractor import data_collector\n",
    "from bert_codes.own_bert_models import *\n",
    "from bert_codes.utils import *\n",
    "from transformers import *\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from sklearn.metrics import accuracy_score,f1_score\n",
    "from utils_function import mapping_to_actual_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 GPU(s) available.\n",
      "We will use the GPU: Tesla P100-PCIE-16GB\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():    \n",
    "\t# Tell PyTorch to use the GPU.    \n",
    "\tdevice = torch.device(\"cuda\")\n",
    "\tprint('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\tprint('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "# If not...\n",
    "else:\n",
    "\tprint('No GPU available, using the CPU instead.')\n",
    "\tdevice = torch.device(\"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={\n",
    "    'max_length':128,\n",
    "    'path_files': 'models_saved/mbert_fearspeech/',\n",
    "    'what_bert':'normal',\n",
    "    'batch_size':32,\n",
    "    'is_train':True,\n",
    "    'learning_rate':2e-5,\n",
    "    'epsilon':1e-8,\n",
    "    'random_seed':2020,\n",
    "    'weights':[1.0,9.0],\n",
    "    'epochs':5\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BERT_for_inference(params,total_data=None):\n",
    "    tokenizer = BertTokenizer.from_pretrained(params['path_files'], do_lower_case=False)\n",
    "    model=select_model(params['what_bert'],params['path_files'],params['weights'])\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    all_sentences = total_data.text    \n",
    "    input_total_ids,att_masks_total=combine_features(all_sentences,tokenizer,params['max_length'],\n",
    "                                                     take_pair=False,take_target=False)\n",
    "    train_dataloader = return_dataloader_inference(input_total_ids,att_masks_total,batch_size=params['batch_size'],is_train=False)\n",
    "    \n",
    "    softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    pred_labels=[]\n",
    "    pred_probab=[]\n",
    "    \n",
    "    for step, batch in tqdm(enumerate(train_dataloader)):\n",
    "        # Add batch to GPU\n",
    "        t0 = time.time()\n",
    "\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask = batch\n",
    "        # Telling the model not to compute or store gradients, saving memory and\n",
    "        # speeding up validation\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(b_input_ids, \n",
    "                            token_type_ids=None, \n",
    "                            attention_mask=b_input_mask)\n",
    "\n",
    "        logits = outputs[0]\n",
    "        logits = softmax(logits)\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        # Accumulate the total accuracy.\n",
    "        pred_labels+=list(np.argmax(logits, axis=1).flatten())\n",
    "        pred_probab+=list([ele[1] for ele in logits])\n",
    "        \n",
    "        # Track the number of batches\n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    total_data['preds']=pred_labels\n",
    "    total_data['pred_probab']=pred_probab\n",
    "    print(\" Test took: {:}\".format(format_time(time.time() - t0)))\n",
    "    return total_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data=pd.read_pickle('../../Data/data_to_be_annotated.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data_filter=total_data[total_data['keywords_count']>=1]\n",
    "total_data_left=total_data[total_data['keywords_count']<1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47748"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(total_data_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0427 19:34:56.639440 140362865473344 tokenization_utils.py:335] Model name 'models_saved/mbert_fearspeech/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming 'models_saved/mbert_fearspeech/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0427 19:34:56.641127 140362865473344 tokenization_utils.py:364] Didn't find file models_saved/mbert_fearspeech/added_tokens.json. We won't load it.\n",
      "I0427 19:34:56.642192 140362865473344 tokenization_utils.py:416] loading file models_saved/mbert_fearspeech/vocab.txt\n",
      "I0427 19:34:56.642888 140362865473344 tokenization_utils.py:416] loading file None\n",
      "I0427 19:34:56.643572 140362865473344 tokenization_utils.py:416] loading file models_saved/mbert_fearspeech/special_tokens_map.json\n",
      "I0427 19:34:56.644270 140362865473344 tokenization_utils.py:416] loading file models_saved/mbert_fearspeech/tokenizer_config.json\n",
      "I0427 19:34:56.767083 140362865473344 configuration_utils.py:231] loading configuration file models_saved/mbert_fearspeech/config.json\n",
      "I0427 19:34:56.768495 140362865473344 configuration_utils.py:256] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 105879\n",
      "}\n",
      "\n",
      "I0427 19:34:56.769618 140362865473344 modeling_utils.py:438] loading weights file models_saved/mbert_fearspeech/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing in fear\n",
      "Input shape before truncating [[101, 100, 100, 100, 100, 100, 11483, 70843, 564, 35877, 102, 564, 117, 577, 90471, 100, 100, 571, 11231, 23562, 118, 23507, 38688, 100, 100, 102], [101, 100, 11483, 100, 100, 100, 100, 100, 11263, 569, 73183, 10949, 100, 571, 11231, 23562, 100, 100, 31984, 100, 100, 11483, 100, 37208, 13901, 100, 100, 45729, 42516, 12334, 100, 571, 11231, 23562, 100, 95358, 39509, 11483, 100, 100, 73349, 13060, 100, 95200, 118, 100, 100, 11263, 36533, 21671, 13328, 100, 21426, 532, 78135, 566, 13060, 14870, 102, 100, 100, 100, 100, 12334, 100, 100, 58106, 100, 100, 11142, 29521, 100, 95200, 118, 100, 100, 100, 170, 11677, 31519, 142, 10534, 142, 11291, 10285, 142, 12873, 44779, 10390, 142, 21809, 100, 100, 100, 44779, 10390, 142, 21809, 100, 100, 10944, 100, 18262, 100, 100, 102], [101, 100, 117, 100, 117, 100, 100, 100, 100, 20574, 100, 100, 102, 564, 532, 14256, 21310, 11483, 100, 100, 11263, 100, 119, 100, 15807, 100, 102], [101, 19142, 100, 100, 19294, 100, 12334, 100, 100, 117, 30441, 118, 102, 100, 15807, 11231, 29521, 554, 94800, 100, 100, 100, 119, 100, 57533, 10944, 102], [101, 100, 100, 10108, 102, 100, 100, 102]]\n",
      "(47748, 128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1493it [03:56,  6.33it/s]\n",
      "/home/punyajoy/.conda/envs/punyajoy_gpu/lib/python3.7/site-packages/ipykernel_launcher.py:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/punyajoy/.conda/envs/punyajoy_gpu/lib/python3.7/site-packages/ipykernel_launcher.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Test took: 0:00:05\n"
     ]
    }
   ],
   "source": [
    "total_dataframe=BERT_for_inference(params,total_data_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/punyajoy/.conda/envs/punyajoy_gpu/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/punyajoy/.conda/envs/punyajoy_gpu/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "total_data_left['preds']=list(np.zeros((len(total_data_left)),dtype=int))\n",
    "total_data_left['pred_probab']=list(np.zeros((len(total_data_left)),dtype=float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=pd.concat([total_dataframe,total_data_left],axis=0,sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keywords_count</th>\n",
       "      <th>pred_probab</th>\n",
       "      <th>preds</th>\n",
       "      <th>repeated messages</th>\n",
       "      <th>text</th>\n",
       "      <th>three annotator</th>\n",
       "      <th>translated</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>0.010630</td>\n",
       "      <td>0</td>\n",
       "      <td>[14967]</td>\n",
       "      <td>दिग्गज BJP ऩेता ने मुस्लिम को बनाया दामाद, ...</td>\n",
       "      <td>[17c0af28-67ba-4d4d-a42d-75c12a47484c, 4af38fe...</td>\n",
       "      <td>Veteran BJP leader made Muslim son-in-law, Mod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>1</td>\n",
       "      <td>0.377079</td>\n",
       "      <td>0</td>\n",
       "      <td>[1416733, 1593926]</td>\n",
       "      <td>14 फ़रवरी को पुलवामा में हुए आतंकी हमले का बदल...</td>\n",
       "      <td>[35a4bb0c-447e-4aaf-9143-c6641041513f, 9d5aea6...</td>\n",
       "      <td>On February 14, Narendra Modi took the revenge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>1</td>\n",
       "      <td>0.006503</td>\n",
       "      <td>0</td>\n",
       "      <td>[1593860]</td>\n",
       "      <td>: अमेरिका, ब्रिटेन, फ्रांस ने UN में दिया आतं...</td>\n",
       "      <td>[3dd88c74-f576-4c4e-a8f2-74621846366e, c99e0d4...</td>\n",
       "      <td>: America, Britain, France gave a proposal to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>1</td>\n",
       "      <td>0.118407</td>\n",
       "      <td>0</td>\n",
       "      <td>[276271]</td>\n",
       "      <td>: भारतीय एक्शन के बाद PAK पर सख्त अमेरिका, कह...</td>\n",
       "      <td>[c44cc375-b7ea-4371-bc00-716fda661aa7, a178c3a...</td>\n",
       "      <td>: Strict America on PAK after Indian action, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>1</td>\n",
       "      <td>0.010218</td>\n",
       "      <td>0</td>\n",
       "      <td>[616176]</td>\n",
       "      <td>Launching Ceremony of Quranic Encyclopedia ht...</td>\n",
       "      <td>[b3a926b5-d17e-4256-946f-99213973ce2b, ffdca11...</td>\n",
       "      <td>Launching Ceremony of Quranic Encyclopedia ht...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     keywords_count  pred_probab  preds   repeated messages  \\\n",
       "id                                                            \n",
       "24                1     0.010630      0             [14967]   \n",
       "176               1     0.377079      0  [1416733, 1593926]   \n",
       "183               1     0.006503      0           [1593860]   \n",
       "201               1     0.118407      0            [276271]   \n",
       "371               1     0.010218      0            [616176]   \n",
       "\n",
       "                                                  text  \\\n",
       "id                                                       \n",
       "24      दिग्गज BJP ऩेता ने मुस्लिम को बनाया दामाद, ...   \n",
       "176   14 फ़रवरी को पुलवामा में हुए आतंकी हमले का बदल...   \n",
       "183   : अमेरिका, ब्रिटेन, फ्रांस ने UN में दिया आतं...   \n",
       "201   : भारतीय एक्शन के बाद PAK पर सख्त अमेरिका, कह...   \n",
       "371   Launching Ceremony of Quranic Encyclopedia ht...   \n",
       "\n",
       "                                       three annotator  \\\n",
       "id                                                       \n",
       "24   [17c0af28-67ba-4d4d-a42d-75c12a47484c, 4af38fe...   \n",
       "176  [35a4bb0c-447e-4aaf-9143-c6641041513f, 9d5aea6...   \n",
       "183  [3dd88c74-f576-4c4e-a8f2-74621846366e, c99e0d4...   \n",
       "201  [c44cc375-b7ea-4371-bc00-716fda661aa7, a178c3a...   \n",
       "371  [b3a926b5-d17e-4256-946f-99213973ce2b, ffdca11...   \n",
       "\n",
       "                                            translated  \n",
       "id                                                      \n",
       "24   Veteran BJP leader made Muslim son-in-law, Mod...  \n",
       "176  On February 14, Narendra Modi took the revenge...  \n",
       "183  : America, Britain, France gave a proposal to ...  \n",
       "201  : Strict America on PAK after Indian action, s...  \n",
       "371   Launching Ceremony of Quranic Encyclopedia ht...  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#total_data_to_annnotate=pd.read_pickle('../../Data/data_to_be_annotated.pkl')\n",
    "actual_data=pd.read_csv('../../Data/new_data_lang_without_spam_translated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 975989/975989 [01:23<00:00, 11707.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0, 0.0), (1, 0, 0.0), (2, 0, 0.0), (3, 0, 0.0), (4, 0, 0.0)]\n"
     ]
    }
   ],
   "source": [
    "temp2=mapping_to_actual_data(temp,actual_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23232"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(temp2[temp2['pred_probabrobab']>0.8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp2.to_csv('../../Data/new_data_lang_without_spam_translated_BERT_pred.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-punyajoy_gpu] *",
   "language": "python",
   "name": "conda-env-.conda-punyajoy_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

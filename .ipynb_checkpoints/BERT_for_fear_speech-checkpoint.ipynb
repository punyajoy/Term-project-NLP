{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_codes.feature_generation import combine_features,return_dataloader,return_cnngru_dataloader\n",
    "from bert_codes.data_extractor import data_collector\n",
    "from bert_codes.own_bert_models import *\n",
    "from bert_codes.utils import *\n",
    "from transformers import *\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 GPU(s) available.\n",
      "We will use the GPU: Tesla P100-PCIE-16GB\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():    \n",
    "\t# Tell PyTorch to use the GPU.    \n",
    "\tdevice = torch.device(\"cuda\")\n",
    "\tprint('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\tprint('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "# If not...\n",
    "else:\n",
    "\tprint('No GPU available, using the CPU instead.')\n",
    "\tdevice = torch.device(\"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot have number of splits n_splits=10 greater than the number of samples: n_samples=4.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-115-9ea246fce179>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mskf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mStratifiedKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mtrain_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mskf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"TRAIN:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TEST:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/punyajoy_gpu/lib/python3.7/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    331\u001b[0m                 (\"Cannot have number of splits n_splits={0} greater\"\n\u001b[1;32m    332\u001b[0m                  \" than the number of samples: n_samples={1}.\")\n\u001b[0;32m--> 333\u001b[0;31m                 .format(self.n_splits, n_samples))\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot have number of splits n_splits=10 greater than the number of samples: n_samples=4."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "X = [[1, 2], [3, 4], [1, 2], [3, 4]]\n",
    "y = [0, 0, 1, 1]\n",
    "skf = StratifiedKFold(n_splits=2)\n",
    "skf.get_n_splits(X, y)\n",
    "\n",
    "\n",
    "skf=StratifiedKFold(n_splits=10, random_state=None, shuffle=False)\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={\n",
    "    'max_length':128,\n",
    "    'path_files': '../../multilingual_hatespeech/multilingual_bert',\n",
    "    'what_bert':'normal',\n",
    "    'batch_size':32,\n",
    "    'is_train':True,\n",
    "    'learning_rate':2e-5,\n",
    "    'epsilon':1e-8,\n",
    "    'random_seed':2020,\n",
    "    'epochs':5\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Eval_phase(params,test_loader,which_files='test',model=None):\n",
    "    model.eval()\n",
    "    print(\"Running eval on \",which_files,\"...\")\n",
    "    t0 = time.time()\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    # Tracking variables \n",
    "    eval_loss=0.0\n",
    "    true_labels=[]\n",
    "    pred_labels=[]\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in test_dataloader:\n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # Telling the model not to compute or store gradients, saving memory and\n",
    "        # speeding up validation\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(b_input_ids, \n",
    "                            token_type_ids=None, \n",
    "                            attention_mask=b_input_mask)\n",
    "\n",
    "        logits = outputs[0]\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        # Accumulate the total accuracy.\n",
    "        pred_labels+=list(np.argmax(logits, axis=1).flatten())\n",
    "        true_labels+=list(label_ids.flatten())\n",
    "\n",
    "        # Track the number of batches\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    testf1=f1_score(true_labels, pred_labels, average='macro')\n",
    "    testacc=accuracy_score(true_labels,pred_labels)\n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    print(\" Accuracy: {0:.2f}\".format(testacc))\n",
    "    print(\" Fscore: {0:.2f}\".format(testf1))\n",
    "    print(\" Test took: {:}\".format(format_time(time.time() - t0)))\n",
    "    return testf1,testacc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_bert(params):\n",
    "    total_data=pd.read_csv('Total_data_annotated.csv')\n",
    "    all_sentences = total_data.text\n",
    "    all_labels=total_data.label\n",
    "    print('Loading BERT tokenizer...')\n",
    "    tokenizer = BertTokenizer.from_pretrained(params['path_files'], do_lower_case=False)\n",
    "    input_total_ids,att_masks_total=combine_features(all_sentences,tokenizer,params['max_length'],\n",
    "                                                     take_pair=False,take_target=False)\n",
    "    \n",
    "    ###optimizer\n",
    "    \n",
    "        \n",
    "    skf=StratifiedKFold(n_splits=10, random_state=params['random_seed'], shuffle=False)\n",
    "    for train_index, test_index in skf.split(input_total_ids, all_labels):\n",
    "        print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        input_train_ids,att_masks_train,labels_train=input_total_ids[train_index],att_masks_total[train_index],all_labels[train_index]\n",
    "        input_val_ids,att_masks_val,labels_val=input_total_ids[test_index],att_masks_total[test_index],all_labels[test_index]\n",
    "        \n",
    "        model=select_model(params['what_bert'],params['path_files'])\n",
    "        model.cuda()\n",
    "        optimizer = AdamW(model.parameters(),\n",
    "                      lr = params['learning_rate'], # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                      eps = params['epsilon'] # args.adam_epsilon  - default is 1e-8.\n",
    "                    )\n",
    "\n",
    "        \n",
    "        train_dataloader = return_dataloader(input_train_ids,labels_train,att_masks_train,batch_size=params['batch_size'],is_train=params['is_train'])\n",
    "        validation_dataloader=return_dataloader(input_val_ids,labels_val,att_masks_val,batch_size=params['batch_size'],is_train=False)\n",
    "        total_steps = len(train_dataloader) * params['epochs']\n",
    "\n",
    "        # Create the learning rate scheduler.\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                                    num_warmup_steps = int(total_steps/10), # Default value in run_glue.py\n",
    "                                                    num_training_steps = total_steps)\n",
    "\n",
    "        # Set the seed value all over the place to make this reproducible.\n",
    "        fix_the_random(seed_val = params['random_seed'])\n",
    "        # Store the averaggit pull origin master --allow-unrelated-historiese loss after each epoch so we can plot them.\n",
    "        loss_values = []\n",
    "\n",
    "        bert_model = params['path_files']\n",
    "        best_val_fscore=0\n",
    "        best_test_fscore=0\n",
    "        for epoch_i in range(0, params['epochs']):\n",
    "            print(\"\")\n",
    "            print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, params['epochs']))\n",
    "            print('Training...')\n",
    "\n",
    "            # Measure how long the training epoch takes.\n",
    "            t0 = time.time()\n",
    "\n",
    "            # Reset the total loss for this epoch.\n",
    "            total_loss = 0\n",
    "            model.train()\n",
    "\n",
    "            # For each batch of training data...\n",
    "            for step, batch in tqdm(enumerate(train_dataloader)):\n",
    "\n",
    "                # Progress update every 40 batches.\n",
    "                if step % 40 == 0 and not step == 0:\n",
    "                    # Calculate elapsed time in minutes.\n",
    "                    elapsed = format_time(time.time() - t0)\n",
    "                # `batch` contains three pytorch tensors:\n",
    "                #   [0]: input ids \n",
    "                #   [1]: attention masks\n",
    "                #   [2]: labels \n",
    "                b_input_ids = batch[0].to(device)\n",
    "                b_input_mask = batch[1].to(device)\n",
    "                b_labels = batch[2].to(device)\n",
    "                # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "                model.zero_grad()        \n",
    "\n",
    "                outputs = model(b_input_ids, \n",
    "                            token_type_ids=None, \n",
    "                            attention_mask=b_input_mask, \n",
    "                            labels=b_labels)\n",
    "\n",
    "                # The call to `model` always returns a tuple, so we need to pull the \n",
    "                # loss value out of the tuple.\n",
    "                loss = outputs[0]\n",
    "                # if(params['logging']=='neptune'):\n",
    "                # \tneptune.log_metric('batch_loss',loss)\n",
    "                # Accumulate the training loss over all of the batches so that we can\n",
    "                # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "                # single value; the `.item()` function just returns the Python value \n",
    "                # from the tensor.\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                # Perform a backward pass to calculate the gradients.\n",
    "                loss.backward()\n",
    "\n",
    "                # Clip the norm of the gradients to 1.0.\n",
    "                # This is to help prevent the \"exploding gradients\" problem.\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                # Update parameters and take a step using the computed gradient.\n",
    "                # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "                # modified based on their gradients, the learning rate, etc.\n",
    "                optimizer.step()\n",
    "                # Update the learning rate.\n",
    "                scheduler.step()\n",
    "\n",
    "            # Calculate the average loss over the training data.\n",
    "            avg_train_loss = total_loss / len(train_dataloader)\n",
    "            train_fscore,train_accuracy=Eval_phase(params,'train',model)\n",
    "            print('avg_train_loss',avg_train_loss)\n",
    "            print('train_fscore',train_fscore)\n",
    "            print('train_accuracy',train_accuracy)\n",
    "            # Store the loss value for plotting the learning curve.\n",
    "            loss_values.append(avg_train_loss)\n",
    "            val_fscore,val_accuracy=Eval_phase(params,'val',model)\t\t\n",
    "            #Report the final accuracy for this validation run.\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I \n",
      "re\n"
     ]
    }
   ],
   "source": [
    "sent=\"I go here\"\n",
    "\n",
    "front_sent=sent[0:2]\n",
    "back_sent=sent[-2:]\n",
    "print(front_sent)\n",
    "print(back_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0426 18:27:41.718567 139787478947648 tokenization_utils.py:335] Model name '../../multilingual_hatespeech/multilingual_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '../../multilingual_hatespeech/multilingual_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0426 18:27:41.719625 139787478947648 tokenization_utils.py:364] Didn't find file ../../multilingual_hatespeech/multilingual_bert/added_tokens.json. We won't load it.\n",
      "I0426 18:27:41.720625 139787478947648 tokenization_utils.py:416] loading file ../../multilingual_hatespeech/multilingual_bert/vocab.txt\n",
      "I0426 18:27:41.721345 139787478947648 tokenization_utils.py:416] loading file None\n",
      "I0426 18:27:41.722027 139787478947648 tokenization_utils.py:416] loading file ../../multilingual_hatespeech/multilingual_bert/special_tokens_map.json\n",
      "I0426 18:27:41.722693 139787478947648 tokenization_utils.py:416] loading file ../../multilingual_hatespeech/multilingual_bert/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n",
      "tokenizing in fear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0426 18:27:43.344985 139787478947648 configuration_utils.py:231] loading configuration file ../../multilingual_hatespeech/multilingual_bert/config.json\n",
      "I0426 18:27:43.346127 139787478947648 configuration_utils.py:256] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.3,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 105879\n",
      "}\n",
      "\n",
      "I0426 18:27:43.347180 139787478947648 modeling_utils.py:438] loading weights file ../../multilingual_hatespeech/multilingual_bert/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape before truncating [[101, 57533, 14473, 100, 31984, 10225, 18782, 24800, 10824, 11483, 100, 100, 100, 576, 19713, 13328, 14187, 117, 100, 100, 10944, 70142, 100, 100, 10944, 15721, 11142, 100, 100, 100, 13492, 100, 100, 10944, 26347, 100, 100, 60449, 14187, 117, 100, 100, 11142, 100, 100, 11483, 100, 14187, 119, 119, 119, 100, 100, 554, 21304, 29988, 100, 100, 37955, 100, 100, 100, 15721, 117, 102, 582, 100, 119, 119, 119, 119, 119, 119, 119, 100, 100, 100, 100, 49004, 579, 11231, 25500, 119, 119, 119, 119, 14500, 533, 21377, 41469, 100, 15721, 100, 10944, 53836, 54540, 13142, 119, 119, 136, 136, 136, 136, 100, 100, 17024, 100, 100, 10944, 11384, 67021, 533, 13764, 25695, 11483, 100, 100, 11142, 11384, 100, 100, 11483, 100, 100, 100, 100, 100, 102], [101, 115, 100, 100, 45557, 32152, 12670, 100, 100, 70345, 117, 29931, 100, 100, 117, 46301, 554, 27185, 11059, 100, 115, 100, 100, 117, 45557, 32152, 12670, 100, 100, 100, 100, 100, 100, 11142, 100, 100, 36244, 100, 100, 29931, 100, 100, 100, 100, 10944, 548, 24800, 100, 43892, 100, 591, 100, 100, 13901, 100, 100, 30945, 53393, 40577, 100, 591, 541, 18155, 11059, 102, 563, 29931, 100, 100, 46301, 100, 100, 29231, 100, 591, 100, 100, 11384, 100, 100, 569, 13142, 13163, 14187, 14500, 100, 100, 11483, 100, 100, 100, 76650, 100, 100, 100, 100, 100, 10944, 100, 87355, 10824, 15721, 100, 19294, 100, 100, 53836, 36666, 30993, 532, 32016, 19187, 11059, 95245, 37524, 14619, 591, 100, 19294, 100, 100, 100, 34018, 100, 100, 29231, 591, 102], [101, 115, 13859, 100, 47064, 10944, 100, 11142, 549, 21310, 10824, 579, 12670, 66170, 10944, 566, 93944, 23067, 100, 591, 100, 100, 100, 100, 100, 11483, 100, 100, 94467, 17024, 579, 12670, 87783, 591, 29416, 100, 100, 71073, 11059, 11483, 100, 100, 15807, 100, 100, 100, 21426, 100, 579, 10949, 20705, 13328, 14500, 100, 100, 100, 100, 60449, 100, 78392, 11354, 14187, 100, 21967, 102, 21477, 12670, 100, 591, 115, 115, 100, 120, 11483, 17109, 54210, 21426, 15807, 11231, 100, 49004, 21426, 77455, 13142, 100, 591, 11483, 17109, 54210, 100, 27793, 11231, 10944, 49004, 20505, 11551, 17024, 20787, 591, 115, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 115, 31984, 80652, 100, 100, 100, 19294, 100, 100, 100, 100, 100, 12334, 102], [101, 19142, 100, 100, 10944, 11384, 11142, 100, 100, 117, 100, 100, 100, 16288, 87355, 19086, 13901, 100, 11483, 100, 20574, 100, 100, 548, 73953, 20705, 14668, 11263, 100, 100, 11263, 60653, 100, 119, 100, 100, 19142, 100, 10944, 32095, 20705, 100, 100, 118, 541, 118, 100, 100, 100, 12334, 10944, 31234, 580, 88092, 100, 100, 100, 31984, 19294, 14473, 19142, 100, 100, 45729, 102, 532, 27843, 15132, 11059, 100, 11483, 11263, 14870, 21304, 100, 100, 100, 100, 100, 87452, 19648, 100, 40577, 31234, 119, 100, 100, 100, 117, 100, 100, 100, 100, 100, 100, 100, 100, 90735, 19648, 10944, 12461, 73987, 10944, 100, 10944, 119, 100, 100, 100, 117, 100, 552, 27623, 100, 571, 80702, 43908, 12670, 10944, 90735, 12334, 100, 100, 11483, 100, 100, 100, 102], [101, 11384, 79562, 100, 47417, 100, 100, 22008, 12334, 100, 100, 100, 106, 36798, 100, 100, 47417, 100, 100, 22008, 11263, 100, 15807, 100, 100, 11483, 100, 100, 106, 84845, 10824, 100, 47417, 100, 10944, 100, 100, 100, 53836, 17441, 46036, 100, 106, 100, 100, 118, 122, 119, 47417, 100, 118, 100, 50010, 70574, 100, 100, 100, 100, 591, 100, 18889, 14676, 100, 100, 102, 573, 73282, 15678, 100, 100, 117, 567, 59086, 100, 100, 100, 100, 100, 13492, 100, 106, 33151, 15654, 579, 91680, 44797, 10824, 21967, 10824, 100, 106, 19142, 100, 100, 100, 12334, 100, 13328, 100, 100, 106, 129, 119, 47417, 100, 118, 100, 53380, 12670, 117, 14668, 85066, 10944, 554, 11641, 18660, 566, 50500, 100, 11263, 100, 100, 26235, 100, 106, 100, 118, 102]]\n",
      "(231, 128)\n",
      "TRAIN: [ 17  20  22  23  25  26  28  30  31  32  34  35  36  37  38  39  40  41\n",
      "  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59\n",
      "  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77\n",
      "  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95\n",
      "  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113\n",
      " 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131\n",
      " 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149\n",
      " 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167\n",
      " 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185\n",
      " 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203\n",
      " 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221\n",
      " 222 223 224 225 226 227 228 229 230] TEST: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 18 19 21 24 27 29 33]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0426 18:27:48.250554 139787478947648 modeling_utils.py:525] Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "I0426 18:27:48.251491 139787478947648 modeling_utils.py:531] Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 5 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [00:04,  1.89it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'which_files' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-127-919527502092>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcross_validate_bert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-125-fdb7222ad640>\u001b[0m in \u001b[0;36mcross_validate_bert\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0;31m# Calculate the average loss over the training data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mavg_train_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0mtrain_fscore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_accuracy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEval_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'avg_train_loss'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mavg_train_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_fscore'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_fscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-117-0df26e3e6a8d>\u001b[0m in \u001b[0;36mEval_phase\u001b[0;34m(params, test_loader, model)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mEval_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Running eval on \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwhich_files\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Put the model in evaluation mode--the dropout layers behave differently\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'which_files' is not defined"
     ]
    }
   ],
   "source": [
    "cross_validate_bert(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data=pd.read_csv('Total_data_annotated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data.label.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-punyajoy_gpu] *",
   "language": "python",
   "name": "conda-env-.conda-punyajoy_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

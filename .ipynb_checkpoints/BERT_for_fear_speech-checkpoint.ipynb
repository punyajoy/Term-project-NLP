{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_codes.feature_generation import combine_features,return_dataloader,return_dataloader_inference,return_cnngru_dataloader\n",
    "from bert_codes.data_extractor import data_collector\n",
    "from bert_codes.own_bert_models import *\n",
    "from bert_codes.utils import *\n",
    "from transformers import *\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from sklearn.metrics import accuracy_score,f1_score\n",
    "from utils_function import mapping_to_actual_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 GPU(s) available.\n",
      "We will use the GPU: Tesla P100-PCIE-16GB\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():    \n",
    "\t# Tell PyTorch to use the GPU.    \n",
    "\tdevice = torch.device(\"cuda\")\n",
    "\tprint('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\tprint('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "# If not...\n",
    "else:\n",
    "\tprint('No GPU available, using the CPU instead.')\n",
    "\tdevice = torch.device(\"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={\n",
    "    'max_length':128,\n",
    "    'path_files': 'models_saved/mbert_fearspeech/',\n",
    "    'what_bert':'normal',\n",
    "    'batch_size':32,\n",
    "    'is_train':True,\n",
    "    'learning_rate':2e-5,\n",
    "    'epsilon':1e-8,\n",
    "    'random_seed':2020,\n",
    "    'weights':[1.0,9.0],\n",
    "    'epochs':5\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BERT_for_inference(params,total_data=None):\n",
    "    tokenizer = BertTokenizer.from_pretrained(params['path_files'], do_lower_case=False)\n",
    "    model=select_model(params['what_bert'],params['path_files'],params['weights'])\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    all_sentences = total_data.message_text    \n",
    "    input_total_ids,att_masks_total=combine_features(all_sentences,tokenizer,params['max_length'],\n",
    "                                                     take_pair=False,take_target=False)\n",
    "    train_dataloader = return_dataloader_inference(input_total_ids,att_masks_total,batch_size=params['batch_size'],is_train=False)\n",
    "    \n",
    "    softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    pred_labels=[]\n",
    "    pred_probab=[]\n",
    "    \n",
    "    for step, batch in tqdm(enumerate(train_dataloader)):\n",
    "        # Add batch to GPU\n",
    "        t0 = time.time()\n",
    "\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask = batch\n",
    "        # Telling the model not to compute or store gradients, saving memory and\n",
    "        # speeding up validation\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(b_input_ids, \n",
    "                            token_type_ids=None, \n",
    "                            attention_mask=b_input_mask)\n",
    "\n",
    "        logits = outputs[0]\n",
    "        logits = softmax(logits)\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        # Accumulate the total accuracy.\n",
    "        pred_labels+=list(np.argmax(logits, axis=1).flatten())\n",
    "        pred_probab+=list([ele[1] for ele in logits])\n",
    "        \n",
    "        # Track the number of batches\n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    total_data['preds']=pred_labels\n",
    "    total_data['pred_probab']=pred_probab\n",
    "    print(\" Test took: {:}\".format(format_time(time.time() - t0)))\n",
    "    return total_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data=pd.read_pickle('../../Data/data_to_be_annotated.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data_filter=total_data[total_data['keywords_count']>1]\n",
    "total_data_left=total_data[total_data['keywords_count']<=1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29091"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(total_data_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0427 18:42:03.274488 140362865473344 tokenization_utils.py:335] Model name 'models_saved/mbert_fearspeech/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming 'models_saved/mbert_fearspeech/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0427 18:42:03.275853 140362865473344 tokenization_utils.py:364] Didn't find file models_saved/mbert_fearspeech/added_tokens.json. We won't load it.\n",
      "I0427 18:42:03.276958 140362865473344 tokenization_utils.py:416] loading file models_saved/mbert_fearspeech/vocab.txt\n",
      "I0427 18:42:03.277593 140362865473344 tokenization_utils.py:416] loading file None\n",
      "I0427 18:42:03.278269 140362865473344 tokenization_utils.py:416] loading file models_saved/mbert_fearspeech/special_tokens_map.json\n",
      "I0427 18:42:03.278928 140362865473344 tokenization_utils.py:416] loading file models_saved/mbert_fearspeech/tokenizer_config.json\n",
      "I0427 18:42:03.379935 140362865473344 configuration_utils.py:231] loading configuration file models_saved/mbert_fearspeech/config.json\n",
      "I0427 18:42:03.381423 140362865473344 configuration_utils.py:256] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 105879\n",
      "}\n",
      "\n",
      "I0427 18:42:03.382547 140362865473344 modeling_utils.py:438] loading weights file models_saved/mbert_fearspeech/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing in fear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape before truncating [[101, 100, 100, 100, 100, 70142, 100, 100, 100, 10944, 100, 100, 591, 100, 100, 100, 70142, 100, 100, 100, 100, 10944, 100, 100, 102, 53836, 11354, 591, 100, 100, 567, 54060, 16129, 100, 100, 100, 100, 100, 100, 100, 100, 100, 15259, 100, 100, 100, 100, 10944, 100, 100, 591, 102], [101, 20696, 100, 11677, 31519, 142, 10534, 142, 11291, 10285, 142, 12873, 100, 118, 84068, 27577, 96807, 11551, 100, 577, 37120, 15678, 100, 100, 102, 573, 10824, 117, 100, 100, 17553, 142, 10171, 142, 22722, 100, 577, 11231, 11483, 100, 100, 100, 13492, 557, 10949, 33912, 15674, 100, 100, 102], [101, 14372, 100, 11384, 533, 13764, 25695, 100, 100, 100, 100, 19294, 100, 569, 47786, 10949, 21426, 100, 30441, 67715, 10855, 85066, 569, 47786, 11231, 117, 81508, 39509, 100, 547, 37107, 10949, 14500, 107, 571, 11231, 23562, 11483, 576, 68309, 566, 100, 107, 591, 88607, 47064, 11354, 37208, 100, 100, 591, 14965, 64581, 142, 12828, 142, 10171, 142, 150, 102, 61998, 10123, 118, 19452, 48357, 10285, 142, 12828, 142, 10171, 142, 14965, 64581, 142, 19452, 14965, 64581, 142, 12828, 142, 10171, 142, 18919, 62319, 10107, 14965, 64581, 142, 12828, 142, 10171, 142, 18919, 62319, 10107, 22153, 54170, 10422, 142, 12828, 570, 25018, 25662, 100, 21426, 15807, 40577, 13163, 58106, 27596, 45729, 533, 15674, 100, 100, 569, 47786, 11231, 102], [101, 14372, 100, 11384, 533, 13764, 25695, 100, 100, 100, 100, 19294, 100, 569, 47786, 10949, 21426, 100, 30441, 67715, 10855, 85066, 569, 47786, 11231, 117, 81508, 39509, 100, 547, 37107, 10949, 14500, 107, 571, 11231, 23562, 11483, 576, 68309, 566, 100, 107, 591, 88607, 47064, 11354, 37208, 100, 100, 591, 14965, 64581, 142, 12828, 142, 10171, 142, 150, 102, 61998, 10123, 118, 19452, 48357, 10285, 142, 12828, 142, 10171, 142, 14965, 64581, 142, 19452, 14965, 64581, 142, 12828, 142, 10171, 142, 18919, 62319, 10107, 14965, 64581, 142, 12828, 142, 10171, 142, 18919, 62319, 10107, 22153, 54170, 10422, 142, 12828, 570, 25018, 25662, 100, 21426, 15807, 40577, 13163, 58106, 27596, 45729, 533, 15674, 100, 100, 569, 47786, 11231, 102], [101, 100, 100, 100, 100, 100, 100, 100, 102, 100, 100, 100, 100, 100, 100, 100, 100, 102]]\n",
      "(1000, 128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [00:04,  6.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "total_dataframe=BERT_for_inference(params,total_data_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0.1.1</th>\n",
       "      <th>Unnamed: 0.1.1.1</th>\n",
       "      <th>Unnamed: 0.1.1.1.1</th>\n",
       "      <th>group_id_anonymized</th>\n",
       "      <th>lang</th>\n",
       "      <th>message_text</th>\n",
       "      <th>phone_num_anonymized</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>translated</th>\n",
       "      <th>preds</th>\n",
       "      <th>pred_probab</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>577</td>\n",
       "      <td>hi</td>\n",
       "      <td>*‡§≤‡§ò‡•Å ‡§∏‡§ø‡§Ç‡§ö‡§æ‡§à*  ‡§®‡§ø‡§É‡§∂‡•Å‡§≤‡•ç‡§ï ‡§¨‡•ã‡§∞‡§ø‡§Ç‡§ó ‡§Ø‡•ã‡§ú‡§®‡§æ ‡§π‡•á‡§§‡•Å 55 ‡§ï‡§∞...</td>\n",
       "      <td>178320</td>\n",
       "      <td>1549559608000</td>\n",
       "      <td>* Provision of Rs. 55 crore for minor irrigati...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.005120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2037</td>\n",
       "      <td>hi</td>\n",
       "      <td>*üì∞FR62* *üëâ‡§¶‡§ø‡§≤‡•ç‡§≤‡•Ä-‡§è‡§®‡§∏‡•Ä‡§Ü‡§∞ ‡§Æ‡•á‡§Ç ‡§∂‡§ø‡§Æ‡§≤‡§æ ‡§ú‡•à‡§∏‡•Ä ‡§¨‡§∞‡•ç‡§´‡§¨‡§æ‡§∞...</td>\n",
       "      <td>39877</td>\n",
       "      <td>1549559738000</td>\n",
       "      <td>* üì∞  üì∞FR62 üëâ  * * Shimla-like snowfall in Delh...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.004702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3634</td>\n",
       "      <td>hi</td>\n",
       "      <td>‡§ï‡§≤ ‡§Æ‡•à‡§®‡•á ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§∏‡•á ‡§≤‡§ø‡§´‡•ç‡§ü ‡§Æ‡§æ‡§Å‡§ó‡§æ ‡§â‡§∏‡§ï‡•á ‡§¨‡§æ‡§¶ ‡§ß‡§®‡•ç‡§Ø‡§µ‡§æ...</td>\n",
       "      <td>198635</td>\n",
       "      <td>1549559843000</td>\n",
       "      <td>Yesterday I asked for a lift from a man, after...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.980105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2284</td>\n",
       "      <td>hi</td>\n",
       "      <td>‡§ï‡§≤ ‡§Æ‡•à‡§®‡•á ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§∏‡•á ‡§≤‡§ø‡§´‡•ç‡§ü ‡§Æ‡§æ‡§Å‡§ó‡§æ ‡§â‡§∏‡§ï‡•á ‡§¨‡§æ‡§¶ ‡§ß‡§®‡•ç‡§Ø‡§µ‡§æ...</td>\n",
       "      <td>198635</td>\n",
       "      <td>1549559851000</td>\n",
       "      <td>Yesterday I asked for a lift from a man, after...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.980105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>3700</td>\n",
       "      <td>te</td>\n",
       "      <td>‡∞∏‡∞∞‡±ç‡∞µ‡±á‡∞≤‡±Å ‡∞∏‡±Ç‡∞∏‡∞ø ‡∞Æ‡±Å‡∞∞‡∞ø‡∞∏‡∞ø ‡∞™‡±ã‡∞µ‡∞¶‡±ç‡∞¶‡±Å ‡∞Æ‡∞ø‡∞§‡±ç‡∞∞‡±Å‡∞≤‡±Å ‡∞¨‡∞æ‡∞¨‡±Å ‡∞ú‡∞ø‡∞§‡±ç...</td>\n",
       "      <td>153553</td>\n",
       "      <td>1549559896000</td>\n",
       "      <td>Surveys Don't Surprise Friends</td>\n",
       "      <td>0</td>\n",
       "      <td>0.015361</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1  Unnamed: 0.1.1  Unnamed: 0.1.1.1  \\\n",
       "0           0             0               0               4.0   \n",
       "1           1             1               1               9.0   \n",
       "2           2             2               2              10.0   \n",
       "3           3             3               3              11.0   \n",
       "4           4             4               4              13.0   \n",
       "\n",
       "   Unnamed: 0.1.1.1.1  group_id_anonymized lang  \\\n",
       "0                 4.0                  577   hi   \n",
       "1                 9.0                 2037   hi   \n",
       "2                10.0                 3634   hi   \n",
       "3                11.0                 2284   hi   \n",
       "4                13.0                 3700   te   \n",
       "\n",
       "                                        message_text  phone_num_anonymized  \\\n",
       "0  *‡§≤‡§ò‡•Å ‡§∏‡§ø‡§Ç‡§ö‡§æ‡§à*  ‡§®‡§ø‡§É‡§∂‡•Å‡§≤‡•ç‡§ï ‡§¨‡•ã‡§∞‡§ø‡§Ç‡§ó ‡§Ø‡•ã‡§ú‡§®‡§æ ‡§π‡•á‡§§‡•Å 55 ‡§ï‡§∞...                178320   \n",
       "1  *üì∞FR62* *üëâ‡§¶‡§ø‡§≤‡•ç‡§≤‡•Ä-‡§è‡§®‡§∏‡•Ä‡§Ü‡§∞ ‡§Æ‡•á‡§Ç ‡§∂‡§ø‡§Æ‡§≤‡§æ ‡§ú‡•à‡§∏‡•Ä ‡§¨‡§∞‡•ç‡§´‡§¨‡§æ‡§∞...                 39877   \n",
       "2  ‡§ï‡§≤ ‡§Æ‡•à‡§®‡•á ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§∏‡•á ‡§≤‡§ø‡§´‡•ç‡§ü ‡§Æ‡§æ‡§Å‡§ó‡§æ ‡§â‡§∏‡§ï‡•á ‡§¨‡§æ‡§¶ ‡§ß‡§®‡•ç‡§Ø‡§µ‡§æ...                198635   \n",
       "3  ‡§ï‡§≤ ‡§Æ‡•à‡§®‡•á ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§∏‡•á ‡§≤‡§ø‡§´‡•ç‡§ü ‡§Æ‡§æ‡§Å‡§ó‡§æ ‡§â‡§∏‡§ï‡•á ‡§¨‡§æ‡§¶ ‡§ß‡§®‡•ç‡§Ø‡§µ‡§æ...                198635   \n",
       "4  ‡∞∏‡∞∞‡±ç‡∞µ‡±á‡∞≤‡±Å ‡∞∏‡±Ç‡∞∏‡∞ø ‡∞Æ‡±Å‡∞∞‡∞ø‡∞∏‡∞ø ‡∞™‡±ã‡∞µ‡∞¶‡±ç‡∞¶‡±Å ‡∞Æ‡∞ø‡∞§‡±ç‡∞∞‡±Å‡∞≤‡±Å ‡∞¨‡∞æ‡∞¨‡±Å ‡∞ú‡∞ø‡∞§‡±ç...                153553   \n",
       "\n",
       "       timestamp                                         translated  preds  \\\n",
       "0  1549559608000  * Provision of Rs. 55 crore for minor irrigati...      0   \n",
       "1  1549559738000  * üì∞  üì∞FR62 üëâ  * * Shimla-like snowfall in Delh...      0   \n",
       "2  1549559843000  Yesterday I asked for a lift from a man, after...      1   \n",
       "3  1549559851000  Yesterday I asked for a lift from a man, after...      1   \n",
       "4  1549559896000                   Surveys Don't Surprise Friends        0   \n",
       "\n",
       "   pred_probab  \n",
       "0     0.005120  \n",
       "1     0.004702  \n",
       "2     0.980105  \n",
       "3     0.980105  \n",
       "4     0.015361  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data_to_annnotate=pd.read_pickle('../../Data/data_to_be_annotated.pkl')\n",
    "actual_data=pd.read_csv('../../Data/new_data_lang_without_spam_translated.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-punyajoy_gpu] *",
   "language": "python",
   "name": "conda-env-.conda-punyajoy_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

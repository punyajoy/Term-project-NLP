{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_codes.feature_generation import combine_features,return_dataloader,return_dataloader_inference,return_cnngru_dataloader\n",
    "from bert_codes.data_extractor import data_collector\n",
    "from bert_codes.own_bert_models import *\n",
    "from bert_codes.utils import *\n",
    "from transformers import *\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from sklearn.metrics import accuracy_score,f1_score\n",
    "from utils_function import mapping_to_actual_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 GPU(s) available.\n",
      "We will use the GPU: Tesla P100-PCIE-16GB\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():    \n",
    "\t# Tell PyTorch to use the GPU.    \n",
    "\tdevice = torch.device(\"cuda\")\n",
    "\tprint('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\tprint('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "# If not...\n",
    "else:\n",
    "\tprint('No GPU available, using the CPU instead.')\n",
    "\tdevice = torch.device(\"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={\n",
    "    'max_length':128,\n",
    "    'path_files': 'models_saved/mbert_fearspeech/',\n",
    "    'what_bert':'normal',\n",
    "    'batch_size':32,\n",
    "    'is_train':True,\n",
    "    'learning_rate':2e-5,\n",
    "    'epsilon':1e-8,\n",
    "    'random_seed':2020,\n",
    "    'weights':[1.0,9.0],\n",
    "    'epochs':5\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BERT_for_inference(params,total_data=None):\n",
    "    tokenizer = BertTokenizer.from_pretrained(params['path_files'], do_lower_case=False)\n",
    "    model=select_model(params['what_bert'],params['path_files'],params['weights'])\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    all_sentences = total_data.message_text    \n",
    "    input_total_ids,att_masks_total=combine_features(all_sentences,tokenizer,params['max_length'],\n",
    "                                                     take_pair=False,take_target=False)\n",
    "    train_dataloader = return_dataloader_inference(input_total_ids,att_masks_total,batch_size=params['batch_size'],is_train=False)\n",
    "    \n",
    "    softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    pred_labels=[]\n",
    "    pred_probab=[]\n",
    "    \n",
    "    for step, batch in tqdm(enumerate(train_dataloader)):\n",
    "        # Add batch to GPU\n",
    "        t0 = time.time()\n",
    "\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask = batch\n",
    "        # Telling the model not to compute or store gradients, saving memory and\n",
    "        # speeding up validation\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(b_input_ids, \n",
    "                            token_type_ids=None, \n",
    "                            attention_mask=b_input_mask)\n",
    "\n",
    "        logits = outputs[0]\n",
    "        logits = softmax(logits)\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        # Accumulate the total accuracy.\n",
    "        pred_labels+=list(np.argmax(logits, axis=1).flatten())\n",
    "        pred_probab+=list([ele[1] for ele in logits])\n",
    "        \n",
    "        # Track the number of batches\n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    total_data['preds']=pred_labels\n",
    "    total_data['pred_probab']=pred_probab\n",
    "    print(\" Test took: {:}\".format(format_time(time.time() - t0)))\n",
    "    return total_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data=pd.read_pickle('../../Data/data_to_be_annotated.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data_filter=total_data[total_data['keywords_count']>1]\n",
    "total_data_left=total_data[total_data['keywords_count']<=1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29091"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(total_data_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0427 18:42:03.274488 140362865473344 tokenization_utils.py:335] Model name 'models_saved/mbert_fearspeech/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming 'models_saved/mbert_fearspeech/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0427 18:42:03.275853 140362865473344 tokenization_utils.py:364] Didn't find file models_saved/mbert_fearspeech/added_tokens.json. We won't load it.\n",
      "I0427 18:42:03.276958 140362865473344 tokenization_utils.py:416] loading file models_saved/mbert_fearspeech/vocab.txt\n",
      "I0427 18:42:03.277593 140362865473344 tokenization_utils.py:416] loading file None\n",
      "I0427 18:42:03.278269 140362865473344 tokenization_utils.py:416] loading file models_saved/mbert_fearspeech/special_tokens_map.json\n",
      "I0427 18:42:03.278928 140362865473344 tokenization_utils.py:416] loading file models_saved/mbert_fearspeech/tokenizer_config.json\n",
      "I0427 18:42:03.379935 140362865473344 configuration_utils.py:231] loading configuration file models_saved/mbert_fearspeech/config.json\n",
      "I0427 18:42:03.381423 140362865473344 configuration_utils.py:256] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 105879\n",
      "}\n",
      "\n",
      "I0427 18:42:03.382547 140362865473344 modeling_utils.py:438] loading weights file models_saved/mbert_fearspeech/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing in fear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape before truncating [[101, 100, 100, 100, 100, 70142, 100, 100, 100, 10944, 100, 100, 591, 100, 100, 100, 70142, 100, 100, 100, 100, 10944, 100, 100, 102, 53836, 11354, 591, 100, 100, 567, 54060, 16129, 100, 100, 100, 100, 100, 100, 100, 100, 100, 15259, 100, 100, 100, 100, 10944, 100, 100, 591, 102], [101, 20696, 100, 11677, 31519, 142, 10534, 142, 11291, 10285, 142, 12873, 100, 118, 84068, 27577, 96807, 11551, 100, 577, 37120, 15678, 100, 100, 102, 573, 10824, 117, 100, 100, 17553, 142, 10171, 142, 22722, 100, 577, 11231, 11483, 100, 100, 100, 13492, 557, 10949, 33912, 15674, 100, 100, 102], [101, 14372, 100, 11384, 533, 13764, 25695, 100, 100, 100, 100, 19294, 100, 569, 47786, 10949, 21426, 100, 30441, 67715, 10855, 85066, 569, 47786, 11231, 117, 81508, 39509, 100, 547, 37107, 10949, 14500, 107, 571, 11231, 23562, 11483, 576, 68309, 566, 100, 107, 591, 88607, 47064, 11354, 37208, 100, 100, 591, 14965, 64581, 142, 12828, 142, 10171, 142, 150, 102, 61998, 10123, 118, 19452, 48357, 10285, 142, 12828, 142, 10171, 142, 14965, 64581, 142, 19452, 14965, 64581, 142, 12828, 142, 10171, 142, 18919, 62319, 10107, 14965, 64581, 142, 12828, 142, 10171, 142, 18919, 62319, 10107, 22153, 54170, 10422, 142, 12828, 570, 25018, 25662, 100, 21426, 15807, 40577, 13163, 58106, 27596, 45729, 533, 15674, 100, 100, 569, 47786, 11231, 102], [101, 14372, 100, 11384, 533, 13764, 25695, 100, 100, 100, 100, 19294, 100, 569, 47786, 10949, 21426, 100, 30441, 67715, 10855, 85066, 569, 47786, 11231, 117, 81508, 39509, 100, 547, 37107, 10949, 14500, 107, 571, 11231, 23562, 11483, 576, 68309, 566, 100, 107, 591, 88607, 47064, 11354, 37208, 100, 100, 591, 14965, 64581, 142, 12828, 142, 10171, 142, 150, 102, 61998, 10123, 118, 19452, 48357, 10285, 142, 12828, 142, 10171, 142, 14965, 64581, 142, 19452, 14965, 64581, 142, 12828, 142, 10171, 142, 18919, 62319, 10107, 14965, 64581, 142, 12828, 142, 10171, 142, 18919, 62319, 10107, 22153, 54170, 10422, 142, 12828, 570, 25018, 25662, 100, 21426, 15807, 40577, 13163, 58106, 27596, 45729, 533, 15674, 100, 100, 569, 47786, 11231, 102], [101, 100, 100, 100, 100, 100, 100, 100, 102, 100, 100, 100, 100, 100, 100, 100, 100, 102]]\n",
      "(1000, 128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [00:04,  6.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "total_dataframe=BERT_for_inference(params,total_data_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0.1.1</th>\n",
       "      <th>Unnamed: 0.1.1.1</th>\n",
       "      <th>Unnamed: 0.1.1.1.1</th>\n",
       "      <th>group_id_anonymized</th>\n",
       "      <th>lang</th>\n",
       "      <th>message_text</th>\n",
       "      <th>phone_num_anonymized</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>translated</th>\n",
       "      <th>preds</th>\n",
       "      <th>pred_probab</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>577</td>\n",
       "      <td>hi</td>\n",
       "      <td>*लघु सिंचाई*  निःशुल्क बोरिंग योजना हेतु 55 कर...</td>\n",
       "      <td>178320</td>\n",
       "      <td>1549559608000</td>\n",
       "      <td>* Provision of Rs. 55 crore for minor irrigati...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.005120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2037</td>\n",
       "      <td>hi</td>\n",
       "      <td>*📰FR62* *👉दिल्ली-एनसीआर में शिमला जैसी बर्फबार...</td>\n",
       "      <td>39877</td>\n",
       "      <td>1549559738000</td>\n",
       "      <td>* 📰  📰FR62 👉  * * Shimla-like snowfall in Delh...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.004702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3634</td>\n",
       "      <td>hi</td>\n",
       "      <td>कल मैने एक आदमी से लिफ्ट माँगा उसके बाद धन्यवा...</td>\n",
       "      <td>198635</td>\n",
       "      <td>1549559843000</td>\n",
       "      <td>Yesterday I asked for a lift from a man, after...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.980105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2284</td>\n",
       "      <td>hi</td>\n",
       "      <td>कल मैने एक आदमी से लिफ्ट माँगा उसके बाद धन्यवा...</td>\n",
       "      <td>198635</td>\n",
       "      <td>1549559851000</td>\n",
       "      <td>Yesterday I asked for a lift from a man, after...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.980105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>3700</td>\n",
       "      <td>te</td>\n",
       "      <td>సర్వేలు సూసి మురిసి పోవద్దు మిత్రులు బాబు జిత్...</td>\n",
       "      <td>153553</td>\n",
       "      <td>1549559896000</td>\n",
       "      <td>Surveys Don't Surprise Friends</td>\n",
       "      <td>0</td>\n",
       "      <td>0.015361</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1  Unnamed: 0.1.1  Unnamed: 0.1.1.1  \\\n",
       "0           0             0               0               4.0   \n",
       "1           1             1               1               9.0   \n",
       "2           2             2               2              10.0   \n",
       "3           3             3               3              11.0   \n",
       "4           4             4               4              13.0   \n",
       "\n",
       "   Unnamed: 0.1.1.1.1  group_id_anonymized lang  \\\n",
       "0                 4.0                  577   hi   \n",
       "1                 9.0                 2037   hi   \n",
       "2                10.0                 3634   hi   \n",
       "3                11.0                 2284   hi   \n",
       "4                13.0                 3700   te   \n",
       "\n",
       "                                        message_text  phone_num_anonymized  \\\n",
       "0  *लघु सिंचाई*  निःशुल्क बोरिंग योजना हेतु 55 कर...                178320   \n",
       "1  *📰FR62* *👉दिल्ली-एनसीआर में शिमला जैसी बर्फबार...                 39877   \n",
       "2  कल मैने एक आदमी से लिफ्ट माँगा उसके बाद धन्यवा...                198635   \n",
       "3  कल मैने एक आदमी से लिफ्ट माँगा उसके बाद धन्यवा...                198635   \n",
       "4  సర్వేలు సూసి మురిసి పోవద్దు మిత్రులు బాబు జిత్...                153553   \n",
       "\n",
       "       timestamp                                         translated  preds  \\\n",
       "0  1549559608000  * Provision of Rs. 55 crore for minor irrigati...      0   \n",
       "1  1549559738000  * 📰  📰FR62 👉  * * Shimla-like snowfall in Delh...      0   \n",
       "2  1549559843000  Yesterday I asked for a lift from a man, after...      1   \n",
       "3  1549559851000  Yesterday I asked for a lift from a man, after...      1   \n",
       "4  1549559896000                   Surveys Don't Surprise Friends        0   \n",
       "\n",
       "   pred_probab  \n",
       "0     0.005120  \n",
       "1     0.004702  \n",
       "2     0.980105  \n",
       "3     0.980105  \n",
       "4     0.015361  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data_to_annnotate=pd.read_pickle('../../Data/data_to_be_annotated.pkl')\n",
    "actual_data=pd.read_csv('../../Data/new_data_lang_without_spam_translated.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-punyajoy_gpu] *",
   "language": "python",
   "name": "conda-env-.conda-punyajoy_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
